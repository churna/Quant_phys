{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def download_pdf(pdf_url, save_path):\n",
    "    # Send a GET request to the PDF link\n",
    "    response = requests.get(pdf_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error downloading PDF from {pdf_url}: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    # Save the PDF content to a file\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded PDF to {save_path}\")\n",
    "\n",
    "def get_article_links(url):\n",
    "    # Send GET request to the page\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching page: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all links to the articles (they're inside <div class=\"list-title\">)\n",
    "    links = []\n",
    "    for link in soup.find_all('a', title=\"Abstract\"):\n",
    "        article_id = link['href'].split('/')[-1]  # Extract article ID from the URL\n",
    "        article_url = f\"https://arxiv.org/abs/{article_id}\"  # Full URL for abstract\n",
    "        links.append(article_url)\n",
    "    return links\n",
    "\n",
    "def scrape_multiple_pages(start_page=0, max_pages=5):\n",
    "    # URL of the recent quantum physics submissions (adjusted for pagination)\n",
    "    base_url = 'https://arxiv.org/list/quant-ph/recent?skip={}&show=50'\n",
    "    \n",
    "    # Create a directory to store downloaded PDFs\n",
    "    if not os.path.exists('quant_physics_articles'):\n",
    "        os.makedirs('quant_physics_articles')\n",
    "    \n",
    "    # Loop through the pages and fetch articles\n",
    "    for page_num in range(start_page, max_pages):\n",
    "        print(f\"Scraping page {page_num + 1}...\")\n",
    "        url = base_url.format(page_num * 50)  # Calculate skip based on the page number\n",
    "        article_links = get_article_links(url)\n",
    "        \n",
    "        if not article_links:\n",
    "            print(\"No articles found. Exiting.\")\n",
    "            break\n",
    "        \n",
    "        # Loop through each article link and download the PDF\n",
    "        for article_link in article_links:\n",
    "            print(f\"Processing: {article_link}\")\n",
    "            \n",
    "            # Send GET request to the article's abstract page\n",
    "            response = requests.get(article_link)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error fetching article page: {response.status_code}\")\n",
    "                continue\n",
    "            \n",
    "            # Parse the abstract page to get the article ID (for the PDF URL)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_id = article_link.split('/')[-1]  # Extract article ID\n",
    "            \n",
    "            # Construct the correct PDF URL using the article ID\n",
    "            pdf_url = f\"https://arxiv.org/pdf/{article_id}.pdf\"\n",
    "            \n",
    "            # Define save path for the PDF\n",
    "            save_path = os.path.join('quant_physics_articles', f\"{article_id}.pdf\")\n",
    "            \n",
    "            # Download and save the PDF\n",
    "            download_pdf(pdf_url, save_path)\n",
    "\n",
    "        # Pause to avoid hitting the server too frequently\n",
    "        time.sleep(3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_multiple_pages(start_page=0, max_pages=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: could not parse color space (156 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (311 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (443 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (504 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (689 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (5667 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (5749 0 R)\n",
      "\n",
      "Loaded 71 documents.\n",
      "Index has been saved to D:/LLM/LangChain/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Document\n",
    "\n",
    "# Define the path to the directory containing your documents (e.g., PDFs)\n",
    "pdf_dir = r\"D:/LLM/Quant_phys/quant_physics_files\"\n",
    "\n",
    "# Define the model for embeddings (e.g., 'all-MiniLM-L6-v2')\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Function to extract text from PDFs (optional if your files are PDFs)index\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "# Define a function to wrap text into Document objects\n",
    "def create_document_from_text(text, filename):\n",
    "    return Document(\n",
    "        text=text, \n",
    "        metadata={\"file_name\": filename}  # Optional: Add file name as metadata\n",
    "    )\n",
    "\n",
    "# Extract text from all PDF files in the directory and create Document objects\n",
    "documents = []\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_dir, filename)\n",
    "        pdf_text = extract_text_from_pdf(pdf_path)\n",
    "        document = create_document_from_text(pdf_text, filename)\n",
    "        documents.append(document)\n",
    "\n",
    "# Check how many documents were loaded\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "# Create the embedding model for HuggingFace\n",
    "embed_model = HuggingFaceEmbedding(model_name=model_name)\n",
    "\n",
    "# Create the Vector Store Index from the loaded documents\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# Define the directory where the index will be saved\n",
    "index_directory = r\"D:/LLM/Quant_phys/\"\n",
    "\n",
    "# Save the index to the specified directory\n",
    "index.storage_context.persist(persist_dir=index_directory)\n",
    "\n",
    "# Print success message\n",
    "print(f\"Index has been saved to {index_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded successfully!\n",
      "Loaded index: <llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x0000024FD41ED9D0>\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "\n",
    "# Define the persist directory where index was saved\n",
    "persist_dir = r\"D:/LLM/Quant_phys/\"\n",
    "\n",
    "# Recreate the storage context from the saved directory\n",
    "storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "\n",
    "# Now you can load the index from the storage context\n",
    "loaded_index = load_index_from_storage(storage_context,embed_model=embed_model)\n",
    "\n",
    "# Now the index is ready for querying or further operations\n",
    "print(\"Index loaded successfully!\")\n",
    "print(f\"Loaded index: {loaded_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoherence is the process by which a quantum system loses its coherence due to interactions with its environment, leading to the emergence of classical behavior.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "# Create the query engine with local embeddings\n",
    "query_engine = index.as_query_engine(Open_API_key=openai.api_key)\n",
    "\n",
    "# Example query\n",
    "query = \"What is dechorence?\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
